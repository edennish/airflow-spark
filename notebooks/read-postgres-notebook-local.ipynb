{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/12/29 16:16:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"read-postgres\")\n",
    "         # Add postgres jar\n",
    "         #.config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/postgresql-9.4.1207.jar\")\n",
    "         .config(\"spark.driver.extraClassPath\", \"/usr/local/spark/resources/jars/postgresql-9.4.1207.jar\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark_clt = (SparkSession\n",
    "         .builder\n",
    "         .master(\"spark://spark:7077\")\n",
    "         .appName(\"read-postgres\")\n",
    "         # Add postgres jar\n",
    "         #.config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/postgresql-9.4.1207.jar\")\n",
    "         .config(\"spark.driver.extraClassPath\", \"/usr/local/spark/resources/jars/postgresql-9.4.1207.jar\")\n",
    "         #.enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/29 04:10:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/12/29 04:10:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/12/29 04:10:43 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "21/12/29 04:10:43 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.21.0.4\n",
      "21/12/29 04:10:43 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_clt.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+---------+--------------------+------------+------------+--------------+---------+--------------------+-------+--------------------+------------+--------------------+-----------+\n",
      "|              dag_id|is_paused|is_subdag|is_active|  last_scheduler_run|last_pickled|last_expired|scheduler_lock|pickle_id|             fileloc| owners|         description|default_view|   schedule_interval|root_dag_id|\n",
      "+--------------------+---------+---------+---------+--------------------+------------+------------+--------------+---------+--------------------+-------+--------------------+------------+--------------------+-----------+\n",
      "|spark-hello-world...|     true|    false|     true|2021-12-29 16:16:...|        null|        null|          null|     null|/usr/local/airflo...|airflow|This DAG runs a P...|        null|{\"type\": \"timedel...|       null|\n",
      "|          spark-test|     true|    false|     true|2021-12-29 16:16:...|        null|        null|          null|     null|/usr/local/airflo...|airflow|This DAG runs a s...|        null|{\"type\": \"timedel...|       null|\n",
      "|      spark-postgres|     true|    false|     true|2021-12-29 16:16:...|        null|        null|          null|     null|/usr/local/airflo...|airflow|This DAG is a sam...|        null|{\"type\": \"timedel...|       null|\n",
      "+--------------------+---------+---------+---------+--------------------+------------+------------+--------------+---------+--------------------+-------+--------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_movies = (\n",
    "    spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres/airflow\")\n",
    "    .option(\"dbtable\", \"public.dag\")\n",
    "    .option(\"user\", \"airflow\")\n",
    "    .option(\"password\", \"airflow\")\n",
    "    .load() )\n",
    "\n",
    "df_movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = (\n",
    "    spark_clt.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres/airflow\")\n",
    "    .option(\"dbtable\", \"public.dag\")\n",
    "    .option(\"user\", \"airflow\")\n",
    "    .option(\"password\", \"airflow\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+---------+--------------------+------------+------------+--------------+---------+--------------------+-------+--------------------+------------+--------------------+-----------+\n",
      "|              dag_id|is_paused|is_subdag|is_active|  last_scheduler_run|last_pickled|last_expired|scheduler_lock|pickle_id|             fileloc| owners|         description|default_view|   schedule_interval|root_dag_id|\n",
      "+--------------------+---------+---------+---------+--------------------+------------+------------+--------------+---------+--------------------+-------+--------------------+------------+--------------------+-----------+\n",
      "|spark-hello-world...|     true|    false|     true|2021-12-29 16:17:...|        null|        null|          null|     null|/usr/local/airflo...|airflow|This DAG runs a P...|        null|{\"type\": \"timedel...|       null|\n",
      "|          spark-test|     true|    false|     true|2021-12-29 16:17:...|        null|        null|          null|     null|/usr/local/airflo...|airflow|This DAG runs a s...|        null|{\"type\": \"timedel...|       null|\n",
      "|      spark-postgres|     true|    false|     true|2021-12-29 16:17:...|        null|        null|          null|     null|/usr/local/airflo...|airflow|This DAG is a sam...|        null|{\"type\": \"timedel...|       null|\n",
      "+--------------------+---------+---------+---------+--------------------+------------+------------+--------------+---------+--------------------+-------+--------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = (\n",
    "    spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres/test\")\n",
    "    .option(\"dbtable\", \"public.movies\")\n",
    "    .option(\"user\", \"test\")\n",
    "    .option(\"password\", \"postgres\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = (\n",
    "    spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres/test\")\n",
    "    .option(\"dbtable\", \"public.ratings\")\n",
    "    .option(\"user\", \"test\")\n",
    "    .option(\"password\", \"postgres\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 movies with more ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = df_movies.alias(\"m\")\n",
    "df_ratings = df_ratings.alias(\"r\")\n",
    "\n",
    "df_join = df_ratings.join(df_movies, df_ratings.movieId == df_movies.movieId).select(\"r.*\",\"m.title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_result = (\n",
    "    df_join\n",
    "    .groupBy(\"title\")\n",
    "    .agg(\n",
    "        F.count(\"timestamp\").alias(\"qty_ratings\")\n",
    "        ,F.mean(\"rating\").alias(\"avg_rating\")\n",
    "    )\n",
    "    .sort(F.desc(\"qty_ratings\"))\n",
    "    .limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.coalesce(1).write.format(\"csv\").mode(\"overwrite\").save(\"/home/jovyan/work/data/output_postgres\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = (\n",
    "    spark_clt.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres/test\")\n",
    "    .option(\"dbtable\", \"public.movies\")\n",
    "    .option(\"user\", \"test\")\n",
    "    .option(\"password\", \"postgres\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.registerTempTable(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/26 16:08:52 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `default`.`test_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n+- Project [movieId#27, title#28, genres#29]\n   +- SubqueryAlias test\n      +- Relation[movieId#27,title#28,genres#29] JDBCRelation(public.movies) [numPartitions=1]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_227/2523859461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark_clt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create table test_table as ( select * from test)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `default`.`test_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n+- Project [movieId#27, title#28, genres#29]\n   +- SubqueryAlias test\n      +- Relation[movieId#27,title#28,genres#29] JDBCRelation(public.movies) [numPartitions=1]\n"
     ]
    }
   ],
   "source": [
    "spark_clt.sql(\"create table test_table as ( select * from test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
